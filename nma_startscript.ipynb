{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMA example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script demonstrates how to load in data remotely from our ECoG dataset: https://gui.dandiarchive.org/#/dandiset/000055.\n",
    "\n",
    "To use, go to https://hub.dandiarchive.org/, log in with your Github account, and start a tiny server. Then, create a new jupyter notebook, copy the cells below into it, and run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install natsort\n",
    "%pip install dandi\n",
    "%pip install git+https://github.com/catalystneuro/brunton-lab-to-nwb.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import glob\n",
    "import time\n",
    "import natsort\n",
    "import numpy as np\n",
    "\n",
    "# Imports to load in data remotely\n",
    "from pynwb import NWBHDF5IO\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "from ndx_events import LabeledEvents, AnnotatedEventsTable, Events\n",
    "from nwbwidgets.utils.timeseries import align_by_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_ind = 1\n",
    "n_move_evs = 10\n",
    "tlims = [-1.5, 1.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, select *n_move_evs* movement events across the different recording days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all file paths for 1 participant\n",
    "with DandiAPIClient() as client:\n",
    "    paths = []\n",
    "    for file in client.get_dandiset(\"000055\", \"draft\").get_assets_under_path(''):\n",
    "        paths.append(file.path)\n",
    "paths = natsort.natsorted(paths)\n",
    "fids = [val for val in paths if 'sub-'+str(part_ind).zfill(2) in val]\n",
    "\n",
    "# Get all movement events for all files\n",
    "times_all, fid_lst = [], []\n",
    "for i, fid in enumerate(fids):\n",
    "    with DandiAPIClient() as client:\n",
    "        asset = client.get_dandiset(\"000055\", \"draft\").get_asset_by_path(fid)\n",
    "        s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "\n",
    "    io = NWBHDF5IO(s3_path, mode='r', load_namespaces=False, driver='ros3')\n",
    "    nwb = io.read()\n",
    "\n",
    "    events = nwb.processing[\"behavior\"].data_interfaces[\"ReachEvents\"]\n",
    "    times_all.extend(events.timestamps[:].tolist())\n",
    "    fid_lst.extend([fid]*len(events.timestamps[:]))\n",
    "\n",
    "# Randomly select 100 movement events\n",
    "n_tot_evs = len(times_all)\n",
    "assert n_move_evs <= n_tot_evs\n",
    "ev_inds = np.random.permutation(n_tot_evs)[:n_move_evs]\n",
    "times_all = np.asarray(times_all)[ev_inds]\n",
    "fid_lst = np.asarray(fid_lst)[ev_inds]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment the neural data at each movement event (will take very long for more movement events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "fids_uni = np.unique(fid_lst).tolist()\n",
    "for i, fid in enumerate(fids_uni):\n",
    "    with DandiAPIClient() as client:\n",
    "        asset = client.get_dandiset(\"000055\", \"draft\").get_asset_by_path(fid)\n",
    "        s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "\n",
    "    io = NWBHDF5IO(s3_path, mode='r', load_namespaces=False, driver='ros3')\n",
    "    nwb = io.read()\n",
    "    \n",
    "    times = times_all[fid_lst == fid]\n",
    "    starts = times + tlims[0]\n",
    "    stops = times + tlims[1]\n",
    "    \n",
    "    print(fid)\n",
    "    ep_dat_tmp = align_by_times(nwb.acquisition['ElectricalSeries'], starts, stops)\n",
    "    \n",
    "    if i == 0:\n",
    "        ep_dat = ep_dat_tmp.copy()\n",
    "    else:\n",
    "        ep_dat = np.vstack((ep_dat, ep_dat_tmp.copy()))\n",
    "    \n",
    "print(ep_dat.shape)\n",
    "print('Run time: {0:.2f} sec'.format(time.time()-tstart))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwb_data",
   "language": "python",
   "name": "nwb_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
